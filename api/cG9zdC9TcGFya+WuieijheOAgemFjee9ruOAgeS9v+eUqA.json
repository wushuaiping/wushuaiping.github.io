{"title":"Spark安装 配置 使用","date":"2019-01-09T08:32:58.000Z","slug":"Spark安装、配置、使用","comments":true,"tags":["Spark"],"updated":"2019-05-11T16:57:18.644Z","content":"<h3 id=\"安装\">安装<a href=\"post/Spark安装、配置、使用#安装\"></a></h3><h4 id=\"下载\">下载<a href=\"post/Spark安装、配置、使用#下载\"></a></h4><p>进入<a href=\"https://spark.apache.org/downloads.html\" target=\"_blank\" rel=\"noopener\">spark版本选择页面</a>。挑选合适的版本进行下载。我使用的是2.4.0，一定要注意spark版本与hadoop版本的对应关系。不然会导致不兼容。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir /usr/spark/</span><br><span class=\"line\">$ cd /usr/spark</span><br><span class=\"line\">$ wget http://apache.01link.hk/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz</span><br><span class=\"line\">$ tar xvzf spark-2.4.0-bin-hadoop2.7.tgz</span><br></pre></td></tr></table></figure>\n<h3 id=\"配置\">配置<a href=\"post/Spark安装、配置、使用#配置\"></a></h3><p>因为下载的是2.4.0版本的关系，所以需要配置<code>SPARK_LOCAL_HOSTNAME</code>，不然会出现<a href=\"http://note.youdao.com/noteshare?id=7133ab26c933a967a32a278595e0ae8e\" target=\"_blank\" rel=\"noopener\">这个问题</a>。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp $&#123;SPARK_HOME&#125;/conf/spark-env.sh.template $&#123;SPARK_HOME&#125;/conf/spark-en.sh</span><br><span class=\"line\">$ vim $&#123;SPARK_HOME&#125;/conf/spark-en.sh</span><br></pre></td></tr></table></figure></p>\n<p>然后加入这两个配置：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export SPARK_LOCAL_HOSTNAME=localhost</span><br><span class=\"line\">export JAVA_HOME=$&#123;JAVA_HOME&#125;/jre</span><br></pre></td></tr></table></figure></p>\n<p>修改<code>/etc/profile</code>的配置，加入这些配置之前，确保Scala已经安装。<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#scala env</span></span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:/usr/lib/scala-2.12.8/bin</span><br><span class=\"line\"><span class=\"comment\">#spark env</span></span><br><span class=\"line\"><span class=\"built_in\">export</span> SPARK_HOME=/usr/spark/spark-2.4.0-bin-hadoop2.7</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$PATH</span>:<span class=\"variable\">$SPARK_HOME</span>/bin:<span class=\"variable\">$SPARK_HOME</span>/sbin</span><br></pre></td></tr></table></figure></p>\n<p>运行<code>spark-shell</code>:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@VM_0_14_centos conf]# spark-shell </span><br><span class=\"line\">19/01/03 14:43:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class=\"line\">Setting default log level to &quot;WARN&quot;.</span><br><span class=\"line\">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class=\"line\">Spark context Web UI available at http://localhost:4040</span><br><span class=\"line\">Spark context available as &apos;sc&apos; (master = local[*], app id = local-1546497818653).</span><br><span class=\"line\">Spark session available as &apos;spark&apos;.</span><br><span class=\"line\">Welcome to</span><br><span class=\"line\">      ____              __</span><br><span class=\"line\">     / __/__  ___ _____/ /__</span><br><span class=\"line\">    _\\ \\/ _ \\/ _ `/ __/  &apos;_/</span><br><span class=\"line\">   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.0</span><br><span class=\"line\">      /_/</span><br><span class=\"line\">         </span><br><span class=\"line\">Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_191)</span><br><span class=\"line\">Type in expressions to have them evaluated.</span><br><span class=\"line\">Type :help for more information.</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Spark-Shell-amp-RDD\">Spark Shell &amp; RDD<a href=\"post/Spark安装、配置、使用#Spark-Shell-amp-RDD\"></a></h3><p>将一个文件读入spark，textFile中的字符串参数可读取文件的绝对路径，当然可以读hdfs的路径。</p>\n<h4 id=\"操作HDFS上的文件\">操作HDFS上的文件<a href=\"post/Spark安装、配置、使用#操作HDFS上的文件\"></a></h4><p>先在HDFS上创建一个目录:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hadoop fs -mkdir -p /user/hadoop/input</span><br></pre></td></tr></table></figure></p>\n<p>新建一个<code>input.txt</code>文件:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vim input.txt</span><br><span class=\"line\"></span><br><span class=\"line\">people are not as beautiful as they look, </span><br><span class=\"line\">as they walk or as they talk.</span><br><span class=\"line\">they are only as beautiful  as they love, </span><br><span class=\"line\">as they care as they share.</span><br></pre></td></tr></table></figure></p>\n<p>上传到HDFS中去：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hadoop fs -put input.txt /user/hadoop/input</span><br></pre></td></tr></table></figure></p>\n<p>运行<code>spark-shell</code>，读取该文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scala&gt; val fileinput=sc.textFile(&quot;hdfs://132.232.254.41:9000/usr/hadoop/input/input.txt&quot;)</span><br><span class=\"line\">fileinput: org.apache.spark.rdd.RDD[String] = hdfs://132.232.254.41:9000/usr/hadoop/input/input.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:24</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"测试用，操作本地文件\">测试用，操作本地文件<a href=\"post/Spark安装、配置、使用#测试用，操作本地文件\"></a></h4><p>随便在哪个目录，建一个<code>input.txt</code>文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vim input.txt</span><br><span class=\"line\"></span><br><span class=\"line\">people are not as beautiful as they look, </span><br><span class=\"line\">as they walk or as they talk.</span><br><span class=\"line\">they are only as beautiful  as they love, </span><br><span class=\"line\">as they care as they share.</span><br></pre></td></tr></table></figure></p>\n<p>不要去其他目录，就在当前目录运行<code>spark-shell</code>，然后读取该文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scala&gt; val inputfile = sc.textFile(&quot;input.txt&quot;)</span><br><span class=\"line\">inputfile: org.apache.spark.rdd.RDD[String] = input.txt MapPartitionsRDD[11] at textFile at &lt;console&gt;:24</span><br></pre></td></tr></table></figure></p>\n<p>这里读取的<code>input.txt</code>就是当前目录的文件。当然也可以指定读取某个目录的某个文件，只需要把绝对路径给全就行了。</p>\n<h5 id=\"读取集合元素数\">读取集合元素数<a href=\"post/Spark安装、配置、使用#读取集合元素数\"></a></h5><p>每一行相当于一个数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scala&gt; inputfile.count()</span><br><span class=\"line\">res0: Long = 4</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"获取第一个元素\">获取第一个元素<a href=\"post/Spark安装、配置、使用#获取第一个元素\"></a></h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scala&gt; inputfile.first()</span><br><span class=\"line\">res1: String = &quot;people are not as beautiful as they look, &quot;</span><br></pre></td></tr></table></figure>\n<h5 id=\"把文件中的单词转换为集合\">把文件中的单词转换为集合<a href=\"post/Spark安装、配置、使用#把文件中的单词转换为集合\"></a></h5><p>以” “分隔字符，然后转为一个collect。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scala&gt; val list=inputfile.flatMap(line =&gt; line.split(&quot; &quot;)).collect()</span><br><span class=\"line\">list: Array[String] = Array(people, are, not, as, beautiful, as, they, look,, as, they, walk, or, as, they, talk., they, are, only, as, beautiful, &quot;&quot;, as, they, love,, as, they, care, as, they, share., i, love, china.)</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"返回文件中前2行的数据，得到一个集合\">返回文件中前2行的数据，得到一个集合<a href=\"post/Spark安装、配置、使用#返回文件中前2行的数据，得到一个集合\"></a></h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scala&gt; inputfile.take(2)</span><br><span class=\"line\">res3: Array[String] = Array(&quot;people are not as beautiful as they look, &quot;, as they walk or as they talk.)</span><br></pre></td></tr></table></figure>","prev":{"title":"Spark原理及Java操作Spark","slug":"Spark原理及Java操作"},"next":{"title":"HBase伪分布式搭建","slug":"HBase伪分布式搭建"},"link":"http://wooo.io/post/Spark安装、配置、使用/"}